{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 22:08:49.899123: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-02 22:08:49.919482: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-02 22:08:50.202029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-04-02 22:08:50 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "/home/binhaoma/anaconda3/envs/SpeechGPT/lib/python3.8/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fairseq.models.text_to_speech.vocoder import CodeHiFiGANVocoder\n",
    "import soundfile as sf\n",
    "from typing import List\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import traceback\n",
    "from peft import PeftModel\n",
    "from utils.speech2unit.speech2unit import Speech2Unit\n",
    "import transformers\n",
    "from transformers import AutoConfig, LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2u_dir: str=\"utils/speech2unit/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binhaoma/anaconda3/envs/SpeechGPT/lib/python3.8/site-packages/fairseq/checkpoint_utils.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
      "2025-04-02 22:08:51 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/binhaoma/thinclient_drives/SpeechGPT/speechgpt\n",
      "2025-04-02 22:08:51 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/annl/s2st/data/voxpopuli/mHuBERT/en_es_fr', 'fine_tuning': False, 'labels': ['km'], 'label_dir': '/checkpoint/wnhsu/experiments/hubert/kmeans/mhubert_vp_en_es_fr_it2_400k/en_es_fr.layer9.km500', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2025-04-02 22:08:51 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n",
      "/home/binhaoma/anaconda3/envs/SpeechGPT/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "2025-04-02 22:08:52 | INFO | generate_pseudo_language | TASK CONFIG:\n",
      "{'_name': 'hubert_pretraining', 'data': '/checkpoint/annl/s2st/data/voxpopuli/mHuBERT/en_es_fr', 'fine_tuning': False, 'labels': ['km'], 'label_dir': '/checkpoint/wnhsu/experiments/hubert/kmeans/mhubert_vp_en_es_fr_it2_400k/en_es_fr.layer9.km500', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "/home/binhaoma/anaconda3/envs/SpeechGPT/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "s2u = Speech2Unit(ckpt_dir=s2u_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<581><63><991><470><821><167><462><104><125><491><405><503><853><487><319><914><476><534><74><635><908><52><270><382><268><412><260><323><534><485><948><86><539><757><41><740><585><611><916><726><584><819><80><658><941><9><127><184><341><465><27><418><6><649><131><567><232><807><6><646><345><88><64><6><33><262><823><64><739><335><739><242><610><712><581><665><662><393><145><920><781><191><513><589><331><529><500><683><200><734><105><561><445><333><259><398><813><260><323><321><323><692><521><757><858><498><338><907><351><706><53><208><555><557><403><295><579><728><284><333><885><790><754><515><526><436><340><611><423><673><764><689><555><764><497><375><588><377><824><829><637><859><750><3><382><245><997><588><237><201><674><763><619><118><174><682><588><523><506><544><197><802><376><651><119><965><377>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original line 2\n",
    "data = [\"<581><63><991><470><821><167><462><104><125><491><405><503><853><487><319><914><476><534><74><635><908>\" \\\n",
    "       \"<52><270><382><268><412><260><323><534><485><948><86><539><757><41><740><585><611><916><726><584><819><80><658><941><9>\" \\\n",
    "       \"<127><184><341><465><27><418><6><649><131><567><232><807><6><646><345><88><64><6><33><262><823><64><739><335><739><242><610><712>\"\\\n",
    "        \"<581><665><662><393><145><920><781><191><513><589><331><529><500><683><200><734><105><561><445><333><259><398><813><260><323><321>\"\\\n",
    "        \"<323><692><521><757><858><498><338><907><351><706><53><208><555><557><403><295><579><728><284><333><885><790><754><515><526><436><340>\"\\\n",
    "        \"<611><423><673><764><689><555><764><497><375><588><377><824><829><637><859><750><3><382><245><997><588><237><201><674><763><619><118>\"\\\n",
    "        \"<174><682><588><523><506><544><197><802><376><651><119><965><377>\"]\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[581, 63, 991, 470, 821, 167, 462, 104, 125, 491, 405, 503, 853, 487, 319, 914, 476, 534, 74, 635, 908, 52, 270, 382, 268, 412, 260, 323, 534, 485, 948, 86, 539, 757, 41, 740, 585, 611, 916, 726, 584, 819, 80, 658, 941, 9, 127, 184, 341, 465, 27, 418, 6, 649, 131, 567, 232, 807, 6, 646, 345, 88, 64, 6, 33, 262, 823, 64, 739, 335, 739, 242, 610, 712, 581, 665, 662, 393, 145, 920, 781, 191, 513, 589, 331, 529, 500, 683, 200, 734, 105, 561, 445, 333, 259, 398, 813, 260, 323, 321, 323, 692, 521, 757, 858, 498, 338, 907, 351, 706, 53, 208, 555, 557, 403, 295, 579, 728, 284, 333, 885, 790, 754, 515, 526, 436, 340, 611, 423, 673, 764, 689, 555, 764, 497, 375, 588, 377, 824, 829, 637, 859, 750, 3, 382, 245, 997, 588, 237, 201, 674, 763, 619, 118, 174, 682, 588, 523, 506, 544, 197, 802, 376, 651, 119, 965, 377]\n",
      "167\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# 提取所有 <数字> 中的数字\n",
    "numbers = re.findall(r'<(\\d+)>', data[0])\n",
    "numbers = [int(num) for num in numbers]  # 转换为整数列表（可选）\n",
    "print(numbers)\n",
    "print(len(numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "ckpt_dir = \"./utils/speech2unit/\"\n",
    "km_path = os.path.join(ckpt_dir, \"mhubert_base_vp_en_es_fr_it3_L11_km1000.bin\")\n",
    "class ApplyKmeans(object):\n",
    "    def __init__(self, km_path):\n",
    "        self.km_model = joblib.load(km_path)\n",
    "        self.C_np = self.km_model.cluster_centers_.transpose()\n",
    "        self.Cnorm_np = (self.C_np ** 2).sum(0, keepdims=True)\n",
    "\n",
    "        self.C = torch.from_numpy(self.C_np)\n",
    "        self.Cnorm = torch.from_numpy(self.Cnorm_np)\n",
    "        if torch.cuda.is_available():\n",
    "            self.C = self.C.cuda()\n",
    "            self.Cnorm = self.Cnorm.cuda()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            self.C = self.C.to(x)\n",
    "            self.Cnorm = self.Cnorm.to(x)\n",
    "            dist = (\n",
    "                x.pow(2).sum(1, keepdim=True)\n",
    "                - 2 * torch.matmul(x, self.C)\n",
    "                + self.Cnorm\n",
    "            )\n",
    "            return -dist\n",
    "        # dist.argmin(dim=1).cpu().numpy()\n",
    "        else:\n",
    "            dist = (\n",
    "                (x ** 2).sum(1, keepdims=True)\n",
    "                - 2 * np.matmul(x, self.C_np)\n",
    "                + self.Cnorm_np\n",
    "            )\n",
    "            return -dist\n",
    "        # np.argmin(dist, axis=1)\n",
    "apply_kmeans = ApplyKmeans(km_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc is 0.0\n",
      "Step 0 | Loss: 62.5579\n",
      "acc is 0.0\n",
      "Step 1 | Loss: 56.8030\n",
      "acc is 0.005988023952095809\n",
      "Step 2 | Loss: 52.0376\n",
      "acc is 0.011976047904191617\n",
      "Step 3 | Loss: 42.5880\n",
      "acc is 0.005988023952095809\n",
      "Step 4 | Loss: 33.6500\n",
      "acc is 0.005988023952095809\n",
      "Step 5 | Loss: 29.8540\n",
      "acc is 0.011976047904191617\n",
      "Step 6 | Loss: 27.3316\n",
      "acc is 0.017964071856287425\n",
      "Step 7 | Loss: 25.6074\n",
      "acc is 0.017964071856287425\n",
      "Step 8 | Loss: 24.5431\n",
      "acc is 0.023952095808383235\n",
      "Step 9 | Loss: 23.4552\n",
      "acc is 0.029940119760479042\n",
      "Step 10 | Loss: 22.3216\n",
      "acc is 0.03592814371257485\n",
      "Step 11 | Loss: 21.3388\n",
      "acc is 0.03592814371257485\n",
      "Step 12 | Loss: 20.5098\n",
      "acc is 0.041916167664670656\n",
      "Step 13 | Loss: 19.7312\n",
      "acc is 0.041916167664670656\n",
      "Step 14 | Loss: 18.9646\n",
      "acc is 0.04790419161676647\n",
      "Step 15 | Loss: 18.1500\n",
      "acc is 0.05389221556886228\n",
      "Step 16 | Loss: 17.3168\n",
      "acc is 0.0658682634730539\n",
      "Step 17 | Loss: 16.5246\n",
      "acc is 0.08383233532934131\n",
      "Step 18 | Loss: 15.7374\n",
      "acc is 0.10179640718562874\n",
      "Step 19 | Loss: 14.9167\n",
      "acc is 0.11377245508982035\n",
      "Step 20 | Loss: 14.1305\n",
      "acc is 0.1377245508982036\n",
      "Step 21 | Loss: 13.4638\n",
      "acc is 0.1317365269461078\n",
      "Step 22 | Loss: 12.8750\n",
      "acc is 0.16167664670658682\n",
      "Step 23 | Loss: 12.1844\n",
      "acc is 0.16167664670658682\n",
      "Step 24 | Loss: 11.6503\n",
      "acc is 0.2215568862275449\n",
      "Step 25 | Loss: 10.9659\n",
      "acc is 0.2275449101796407\n",
      "Step 26 | Loss: 10.4798\n",
      "acc is 0.23952095808383234\n",
      "Step 27 | Loss: 9.8360\n",
      "acc is 0.2934131736526946\n",
      "Step 28 | Loss: 9.2447\n",
      "acc is 0.32335329341317365\n",
      "Step 29 | Loss: 8.6828\n",
      "acc is 0.3413173652694611\n",
      "Step 30 | Loss: 8.1278\n",
      "acc is 0.3712574850299401\n",
      "Step 31 | Loss: 7.6483\n",
      "acc is 0.3712574850299401\n",
      "Step 32 | Loss: 7.2456\n",
      "acc is 0.32934131736526945\n",
      "Step 33 | Loss: 7.0661\n",
      "acc is 0.32934131736526945\n",
      "Step 34 | Loss: 7.2473\n",
      "acc is 0.40119760479041916\n",
      "Step 35 | Loss: 6.5326\n",
      "acc is 0.4431137724550898\n",
      "Step 36 | Loss: 5.9419\n",
      "acc is 0.5029940119760479\n",
      "Step 37 | Loss: 5.5389\n",
      "acc is 0.49700598802395207\n",
      "Step 38 | Loss: 5.3432\n",
      "acc is 0.5029940119760479\n",
      "Step 39 | Loss: 4.9377\n",
      "acc is 0.5269461077844312\n",
      "Step 40 | Loss: 4.6357\n",
      "acc is 0.5868263473053892\n",
      "Step 41 | Loss: 4.3234\n",
      "acc is 0.6047904191616766\n",
      "Step 42 | Loss: 4.0360\n",
      "acc is 0.6287425149700598\n",
      "Step 43 | Loss: 3.7737\n",
      "acc is 0.6646706586826348\n",
      "Step 44 | Loss: 3.5103\n",
      "acc is 0.6467065868263473\n",
      "Step 45 | Loss: 3.3281\n",
      "acc is 0.6167664670658682\n",
      "Step 46 | Loss: 3.4178\n",
      "acc is 0.5449101796407185\n",
      "Step 47 | Loss: 4.0521\n",
      "acc is 0.6526946107784432\n",
      "Step 48 | Loss: 3.3966\n",
      "acc is 0.5329341317365269\n",
      "Step 49 | Loss: 3.6696\n",
      "acc is 0.6167664670658682\n",
      "Step 50 | Loss: 3.3942\n",
      "acc is 0.5568862275449101\n",
      "Step 51 | Loss: 3.5461\n",
      "acc is 0.6347305389221557\n",
      "Step 52 | Loss: 3.1620\n",
      "acc is 0.6167664670658682\n",
      "Step 53 | Loss: 2.9818\n",
      "acc is 0.7245508982035929\n",
      "Step 54 | Loss: 2.6365\n",
      "acc is 0.7065868263473054\n",
      "Step 55 | Loss: 2.3823\n",
      "acc is 0.718562874251497\n",
      "Step 56 | Loss: 2.2392\n",
      "acc is 0.7365269461077845\n",
      "Step 57 | Loss: 2.0832\n",
      "acc is 0.7125748502994012\n",
      "Step 58 | Loss: 2.1313\n",
      "acc is 0.7544910179640718\n",
      "Step 59 | Loss: 1.9350\n",
      "acc is 0.7664670658682635\n",
      "Step 60 | Loss: 1.7625\n",
      "acc is 0.7904191616766467\n",
      "Step 61 | Loss: 1.6175\n",
      "acc is 0.8562874251497006\n",
      "Step 62 | Loss: 1.4334\n",
      "acc is 0.8622754491017964\n",
      "Step 63 | Loss: 1.2704\n",
      "acc is 0.844311377245509\n",
      "Step 64 | Loss: 1.1833\n",
      "acc is 0.8143712574850299\n",
      "Step 65 | Loss: 1.1341\n",
      "acc is 0.874251497005988\n",
      "Step 66 | Loss: 1.0336\n",
      "acc is 0.9101796407185628\n",
      "Step 67 | Loss: 0.9031\n",
      "acc is 0.9101796407185628\n",
      "Step 68 | Loss: 0.8462\n",
      "acc is 0.9341317365269461\n",
      "Step 69 | Loss: 0.7848\n",
      "acc is 0.9161676646706587\n",
      "Step 70 | Loss: 0.7717\n",
      "acc is 0.9041916167664671\n",
      "Step 71 | Loss: 0.7756\n",
      "acc is 0.9101796407185628\n",
      "Step 72 | Loss: 0.8037\n",
      "acc is 0.9281437125748503\n",
      "Step 73 | Loss: 0.6689\n",
      "acc is 0.874251497005988\n",
      "Step 74 | Loss: 0.7416\n",
      "acc is 0.8682634730538922\n",
      "Step 75 | Loss: 0.8997\n",
      "acc is 0.9221556886227545\n",
      "Step 76 | Loss: 0.6257\n",
      "acc is 0.8802395209580839\n",
      "Step 77 | Loss: 0.8330\n",
      "acc is 0.8802395209580839\n",
      "Step 78 | Loss: 0.8367\n",
      "acc is 0.8862275449101796\n",
      "Step 79 | Loss: 0.6592\n",
      "acc is 0.9221556886227545\n",
      "Step 80 | Loss: 0.5977\n",
      "acc is 0.9341317365269461\n",
      "Step 81 | Loss: 0.5461\n",
      "acc is 0.9461077844311377\n",
      "Step 82 | Loss: 0.4950\n",
      "acc is 0.9580838323353293\n",
      "Step 83 | Loss: 0.4728\n",
      "acc is 0.9161676646706587\n",
      "Step 84 | Loss: 0.5205\n",
      "acc is 0.9461077844311377\n",
      "Step 85 | Loss: 0.4527\n",
      "acc is 0.8982035928143712\n",
      "Step 86 | Loss: 0.5280\n",
      "acc is 0.8263473053892215\n",
      "Step 87 | Loss: 0.8804\n",
      "acc is 0.8562874251497006\n",
      "Step 88 | Loss: 0.8145\n",
      "acc is 0.8263473053892215\n",
      "Step 89 | Loss: 1.1234\n",
      "acc is 0.7964071856287425\n",
      "Step 90 | Loss: 0.9960\n",
      "acc is 0.8263473053892215\n",
      "Step 91 | Loss: 0.8706\n",
      "acc is 0.8922155688622755\n",
      "Step 92 | Loss: 0.6627\n",
      "acc is 0.844311377245509\n",
      "Step 93 | Loss: 0.8243\n",
      "acc is 0.8562874251497006\n",
      "Step 94 | Loss: 0.7386\n",
      "acc is 0.8922155688622755\n",
      "Step 95 | Loss: 0.6772\n",
      "acc is 0.8862275449101796\n",
      "Step 96 | Loss: 0.7164\n",
      "acc is 0.8802395209580839\n",
      "Step 97 | Loss: 0.6309\n",
      "acc is 0.9221556886227545\n",
      "Step 98 | Loss: 0.5696\n",
      "acc is 0.9041916167664671\n",
      "Step 99 | Loss: 0.5299\n",
      "acc is 0.9401197604790419\n",
      "Step 100 | Loss: 0.4518\n",
      "acc is 0.9640718562874252\n",
      "Step 101 | Loss: 0.4358\n",
      "acc is 0.9580838323353293\n",
      "Step 102 | Loss: 0.3933\n",
      "acc is 0.9461077844311377\n",
      "Step 103 | Loss: 0.3886\n",
      "acc is 0.9700598802395209\n",
      "Step 104 | Loss: 0.3495\n",
      "acc is 0.9520958083832335\n",
      "Step 105 | Loss: 0.3885\n",
      "acc is 0.9820359281437125\n",
      "Step 106 | Loss: 0.2901\n",
      "acc is 0.9820359281437125\n",
      "Step 107 | Loss: 0.2757\n",
      "acc is 0.9820359281437125\n",
      "Step 108 | Loss: 0.2771\n",
      "acc is 0.9760479041916168\n",
      "Step 109 | Loss: 0.2521\n",
      "acc is 0.9820359281437125\n",
      "Step 110 | Loss: 0.2377\n",
      "acc is 0.9760479041916168\n",
      "Step 111 | Loss: 0.2325\n",
      "acc is 0.9880239520958084\n",
      "Step 112 | Loss: 0.2015\n",
      "acc is 0.9880239520958084\n",
      "Step 113 | Loss: 0.1887\n",
      "acc is 0.9880239520958084\n",
      "Step 114 | Loss: 0.1742\n",
      "acc is 0.9880239520958084\n",
      "Step 115 | Loss: 0.1578\n",
      "acc is 0.9880239520958084\n",
      "Step 116 | Loss: 0.1469\n",
      "acc is 0.9880239520958084\n",
      "Step 117 | Loss: 0.1367\n",
      "acc is 0.9880239520958084\n",
      "Step 118 | Loss: 0.1306\n",
      "acc is 0.9820359281437125\n",
      "Step 119 | Loss: 0.1432\n",
      "acc is 0.9880239520958084\n",
      "Step 120 | Loss: 0.1208\n",
      "acc is 0.9820359281437125\n",
      "Step 121 | Loss: 0.1456\n",
      "acc is 0.9880239520958084\n",
      "Step 122 | Loss: 0.1296\n",
      "acc is 0.9820359281437125\n",
      "Step 123 | Loss: 0.1310\n",
      "acc is 0.9820359281437125\n",
      "Step 124 | Loss: 0.1261\n",
      "acc is 0.9820359281437125\n",
      "Step 125 | Loss: 0.1310\n",
      "acc is 0.9880239520958084\n",
      "Step 126 | Loss: 0.1171\n",
      "acc is 0.9880239520958084\n",
      "Step 127 | Loss: 0.1122\n",
      "acc is 0.9880239520958084\n",
      "Step 128 | Loss: 0.1009\n",
      "acc is 0.9880239520958084\n",
      "Step 129 | Loss: 0.0970\n",
      "acc is 0.9880239520958084\n",
      "Step 130 | Loss: 0.1002\n",
      "acc is 0.9940119760479041\n",
      "Step 131 | Loss: 0.0900\n",
      "acc is 0.9940119760479041\n",
      "Step 132 | Loss: 0.0806\n",
      "acc is 0.9940119760479041\n",
      "Step 133 | Loss: 0.0783\n",
      "acc is 0.9940119760479041\n",
      "Step 134 | Loss: 0.0705\n",
      "acc is 0.9940119760479041\n",
      "Step 135 | Loss: 0.0683\n",
      "acc is 0.9940119760479041\n",
      "Step 136 | Loss: 0.0622\n",
      "acc is 0.9940119760479041\n",
      "Step 137 | Loss: 0.0602\n",
      "acc is 0.9940119760479041\n",
      "Step 138 | Loss: 0.0556\n",
      "acc is 0.9940119760479041\n",
      "Step 139 | Loss: 0.0477\n",
      "acc is 0.9880239520958084\n",
      "Step 140 | Loss: 0.0455\n",
      "acc is 0.9940119760479041\n",
      "Step 141 | Loss: 0.0373\n",
      "acc is 0.9940119760479041\n",
      "Step 142 | Loss: 0.0347\n",
      "acc is 0.9940119760479041\n",
      "Step 143 | Loss: 0.0305\n",
      "acc is 0.9940119760479041\n",
      "Step 144 | Loss: 0.0256\n",
      "acc is 0.9940119760479041\n",
      "Step 145 | Loss: 0.0221\n",
      "acc is 1.0\n",
      "Step 146 | Loss: 0.0180\n",
      "acc is 1.0\n",
      "Step 147 | Loss: 0.0168\n",
      "acc is 1.0\n",
      "Step 148 | Loss: 0.0136\n",
      "acc is 1.0\n",
      "Step 149 | Loss: 0.0126\n",
      "acc is 1.0\n",
      "Step 150 | Loss: 0.0107\n",
      "acc is 1.0\n",
      "Step 151 | Loss: 0.0092\n",
      "acc is 1.0\n",
      "Step 152 | Loss: 0.0085\n",
      "acc is 1.0\n",
      "Step 153 | Loss: 0.0078\n",
      "acc is 1.0\n",
      "Step 154 | Loss: 0.0070\n",
      "acc is 1.0\n",
      "Step 155 | Loss: 0.0062\n",
      "acc is 1.0\n",
      "Step 156 | Loss: 0.0058\n",
      "acc is 1.0\n",
      "Step 157 | Loss: 0.0055\n",
      "acc is 1.0\n",
      "Step 158 | Loss: 0.0052\n",
      "acc is 1.0\n",
      "Step 159 | Loss: 0.0048\n",
      "acc is 1.0\n",
      "Step 160 | Loss: 0.0045\n",
      "acc is 1.0\n",
      "Step 161 | Loss: 0.0043\n",
      "acc is 1.0\n",
      "Step 162 | Loss: 0.0041\n",
      "acc is 1.0\n",
      "Step 163 | Loss: 0.0039\n",
      "acc is 1.0\n",
      "Step 164 | Loss: 0.0036\n",
      "acc is 1.0\n",
      "Step 165 | Loss: 0.0035\n",
      "acc is 1.0\n",
      "Step 166 | Loss: 0.0033\n",
      "acc is 1.0\n",
      "Step 167 | Loss: 0.0032\n",
      "acc is 1.0\n",
      "Step 168 | Loss: 0.0031\n",
      "acc is 1.0\n",
      "Step 169 | Loss: 0.0030\n",
      "acc is 1.0\n",
      "Step 170 | Loss: 0.0029\n",
      "acc is 1.0\n",
      "Step 171 | Loss: 0.0028\n",
      "acc is 1.0\n",
      "Step 172 | Loss: 0.0027\n",
      "acc is 1.0\n",
      "Step 173 | Loss: 0.0026\n",
      "acc is 1.0\n",
      "Step 174 | Loss: 0.0026\n",
      "acc is 1.0\n",
      "Step 175 | Loss: 0.0025\n",
      "acc is 1.0\n",
      "Step 176 | Loss: 0.0024\n",
      "acc is 1.0\n",
      "Step 177 | Loss: 0.0024\n",
      "acc is 1.0\n",
      "Step 178 | Loss: 0.0023\n",
      "acc is 1.0\n",
      "Step 179 | Loss: 0.0023\n",
      "acc is 1.0\n",
      "Step 180 | Loss: 0.0022\n",
      "acc is 1.0\n",
      "Step 181 | Loss: 0.0022\n",
      "acc is 1.0\n",
      "Step 182 | Loss: 0.0021\n",
      "acc is 1.0\n",
      "Step 183 | Loss: 0.0021\n",
      "acc is 1.0\n",
      "Step 184 | Loss: 0.0021\n",
      "acc is 1.0\n",
      "Step 185 | Loss: 0.0020\n",
      "acc is 1.0\n",
      "Step 186 | Loss: 0.0020\n",
      "acc is 1.0\n",
      "Step 187 | Loss: 0.0020\n",
      "acc is 1.0\n",
      "Step 188 | Loss: 0.0019\n",
      "acc is 1.0\n",
      "Step 189 | Loss: 0.0019\n",
      "acc is 1.0\n",
      "Step 190 | Loss: 0.0019\n",
      "acc is 1.0\n",
      "Step 191 | Loss: 0.0018\n",
      "acc is 1.0\n",
      "Step 192 | Loss: 0.0018\n",
      "acc is 1.0\n",
      "Step 193 | Loss: 0.0018\n",
      "acc is 1.0\n",
      "Step 194 | Loss: 0.0018\n",
      "acc is 1.0\n",
      "Step 195 | Loss: 0.0018\n",
      "acc is 1.0\n",
      "Step 196 | Loss: 0.0017\n",
      "acc is 1.0\n",
      "Step 197 | Loss: 0.0017\n",
      "acc is 1.0\n",
      "Step 198 | Loss: 0.0017\n",
      "acc is 1.0\n",
      "Step 199 | Loss: 0.0017\n",
      "acc is 1.0\n",
      "Step 200 | Loss: 0.0017\n",
      "acc is 1.0\n",
      "Step 201 | Loss: 0.0016\n",
      "acc is 1.0\n",
      "Step 202 | Loss: 0.0016\n",
      "acc is 1.0\n",
      "Step 203 | Loss: 0.0016\n",
      "acc is 1.0\n",
      "Step 204 | Loss: 0.0016\n",
      "acc is 1.0\n",
      "Step 205 | Loss: 0.0016\n",
      "acc is 1.0\n",
      "Step 206 | Loss: 0.0016\n",
      "acc is 1.0\n",
      "Step 207 | Loss: 0.0015\n",
      "acc is 1.0\n",
      "Step 208 | Loss: 0.0015\n",
      "acc is 1.0\n",
      "Step 209 | Loss: 0.0015\n",
      "acc is 1.0\n",
      "Step 210 | Loss: 0.0015\n",
      "acc is 1.0\n",
      "Step 211 | Loss: 0.0015\n",
      "acc is 1.0\n",
      "Step 212 | Loss: 0.0015\n",
      "acc is 1.0\n",
      "Step 213 | Loss: 0.0015\n",
      "acc is 1.0\n",
      "Step 214 | Loss: 0.0015\n",
      "acc is 1.0\n",
      "Step 215 | Loss: 0.0014\n",
      "acc is 1.0\n",
      "Step 216 | Loss: 0.0014\n",
      "acc is 1.0\n",
      "Step 217 | Loss: 0.0014\n",
      "acc is 1.0\n",
      "Step 218 | Loss: 0.0014\n",
      "acc is 1.0\n",
      "Step 219 | Loss: 0.0014\n",
      "acc is 1.0\n",
      "Step 220 | Loss: 0.0014\n",
      "acc is 1.0\n",
      "Step 221 | Loss: 0.0014\n",
      "acc is 1.0\n",
      "Step 222 | Loss: 0.0014\n",
      "acc is 1.0\n",
      "Step 223 | Loss: 0.0014\n",
      "acc is 1.0\n",
      "Step 224 | Loss: 0.0013\n",
      "acc is 1.0\n",
      "Step 225 | Loss: 0.0013\n",
      "acc is 1.0\n",
      "Step 226 | Loss: 0.0013\n",
      "acc is 1.0\n",
      "Step 227 | Loss: 0.0013\n",
      "acc is 1.0\n",
      "Step 228 | Loss: 0.0013\n",
      "acc is 1.0\n",
      "Step 229 | Loss: 0.0013\n",
      "acc is 1.0\n",
      "Step 230 | Loss: 0.0013\n",
      "acc is 1.0\n",
      "Step 231 | Loss: 0.0013\n",
      "acc is 1.0\n",
      "Step 232 | Loss: 0.0013\n",
      "acc is 1.0\n",
      "Step 233 | Loss: 0.0013\n",
      "acc is 1.0\n",
      "Step 234 | Loss: 0.0013\n",
      "acc is 1.0\n",
      "Step 235 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 236 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 237 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 238 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 239 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 240 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 241 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 242 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 243 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 244 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 245 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 246 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 247 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 248 | Loss: 0.0012\n",
      "acc is 1.0\n",
      "Step 249 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 250 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 251 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 252 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 253 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 254 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 255 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 256 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 257 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 258 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 259 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 260 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 261 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 262 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 263 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 264 | Loss: 0.0011\n",
      "acc is 1.0\n",
      "Step 265 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 266 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 267 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 268 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 269 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 270 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 271 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 272 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 273 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 274 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 275 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 276 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 277 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 278 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 279 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 280 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 281 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 282 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 283 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 284 | Loss: 0.0010\n",
      "acc is 1.0\n",
      "Step 285 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 286 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 287 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 288 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 289 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 290 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 291 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 292 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 293 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 294 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 295 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 296 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 297 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 298 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 299 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 300 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 301 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 302 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 303 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 304 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 305 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 306 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 307 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 308 | Loss: 0.0009\n",
      "acc is 1.0\n",
      "Step 309 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 310 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 311 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 312 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 313 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 314 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 315 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 316 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 317 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 318 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 319 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 320 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 321 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 322 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 323 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 324 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 325 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 326 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 327 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 328 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 329 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 330 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 331 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 332 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 333 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 334 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 335 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 336 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 337 | Loss: 0.0008\n",
      "acc is 1.0\n",
      "Step 338 | Loss: 0.0007\n",
      "acc is 1.0\n",
      "Step 339 | Loss: 0.0007\n",
      "acc is 1.0\n",
      "Step 340 | Loss: 0.0007\n",
      "acc is 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m acc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m     torchaudio\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt/output_noise.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mnoise_add\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "processed_parts = []\n",
    "part = 'prompt/o1.wav'\n",
    "if os.path.isfile(part.strip()) and os.path.splitext(part.strip())[-1] in [\".wav\", \".flac\", \".mp4\"]:\n",
    "    processed_parts.append(s2u(part.strip(), merged=True))\n",
    "\n",
    "# 我需要168   320 是一步\n",
    "DEVICE = 'cuda'\n",
    "waveform = s2u.feature_reader.read_audio(part).to(DEVICE)\n",
    "ori_shape = waveform.shape[1]/320\n",
    "# 初始 noise\n",
    "noise_len = int((len(numbers) + 1) * 320)\n",
    "\n",
    "# init_noise = torch.rand(1, noise_len, device=DEVICE)\n",
    "# noise = torch.nn.Parameter(init_noise)\n",
    "# optimizer = optim.Adam([noise], lr=5e-2)\n",
    "\n",
    "T = waveform.shape[1]\n",
    "noise_add = torch.nn.Parameter(torch.rand(1, noise_len, device=DEVICE))           # 用于叠加\n",
    "optimizer = torch.optim.Adam([noise_add], lr=1e-2)\n",
    "\n",
    "\n",
    "\n",
    "for step in range(10000):\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    budget = 0.02\n",
    "\n",
    "    with torch.no_grad():\n",
    "        noise_add.clamp_(-budget, budget)\n",
    "\n",
    "    # 提取特征\n",
    "    feat = s2u.feature_reader.get_feats(noise_add)\n",
    "\n",
    "\n",
    "    # 聚类预测\n",
    "    cluster_ids = apply_kmeans(feat)  # list[int]\n",
    "    target = torch.tensor(numbers, dtype=torch.long, device=DEVICE)\n",
    "    # cluster_ids = torch.tensor(cluster_ids).to('cuda')\n",
    "    # one-hot 编码\n",
    "    # num_classes = 1000\n",
    "    # target_onehot = F.one_hot(target, num_classes=num_classes).float()  # shape: [B, 1000]\n",
    "\n",
    "\n",
    "    # 用 MSE 来衡量 label 差异（因为没有 logits）\n",
    "    loss = F.cross_entropy(cluster_ids, target.to('cuda'))\n",
    "    # 也可以用 L1 Loss: F.l1_loss(pred, target)\n",
    "\n",
    "\n",
    "    cluster_final_ids = cluster_ids.argmax(dim=1)  # 每行最大值的索引\n",
    "    acc = (np.array(cluster_final_ids.cpu())==np.array(target.cpu())).sum()/len(target)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'acc is {acc}')\n",
    "    if acc == 1:\n",
    "        torchaudio.save(\"prompt/output_noise.wav\", noise_add.cpu(), sample_rate=16000)\n",
    "\n",
    "\n",
    "    print(f\"Step {step} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpeechGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
